# Pokemon_Classification_Challenge - Given a set of 721 Pokemon, create a model that predicts whether a Pokemon is Legendary or not.

POSSIBLE MODELS
(Note advantages and disadvantages are specific to  binary classification using  a  small-to-medium-sized  data  set  with  relatively  low  dimensionality  compared  to  amount  ofobservations.)

1.Logistic Regression
• Advantages:  Quick to train, simple, gives probability of classification.
• Disadvantages:  Assumes data is linearly separable, must one-hot encode categorical data,does not perform well with correlated features.

2.Support Vector Machine (SVM)
• Advantages:  Quick to train with small amount of data, simple, well suited for binary clas-sification, can determine some metric for probability based on how close a point is to theSVM’s decision boundary (although difficult to measure).
• Disadvantages:   Must  apply  kernel  functions  to  non-linearly  separable  data  and  some-times performs worse on it, must one-hot encode categorical data, better suited for high-dimensional  spaces,  affected  by  correlated  features  with  certain  kernels,  does  not  handle imbalanced data well.

3.Random Forest
• Advantages: Quick to train, demonstrates success in a wide range of task and usually betterthan other classification algorithms, works well with both numerical and categorical data,works well with both linear and non-linear data, will not overfit due to averaging of decisiontrees, easily able to determine feature importance, handles multicollinearity well.
• Disadvantages: Multicollinearity affects outcome of feature importance, biased towards highcardinality features, less necessity to do so but should still one-hot encode categorical data, does not handle imbalanced data well.Note  I  did  not  consider  deep  learning  because  of  the  small  amount  of  observations,  and  I  did  notconsider clustering because I do not believe the data is well separated enough and think a more com-plicated model is necessary.


MODEL DECISION 
I  chose  a  Random  Forest  Classifier. Overall,  random  forests  have  demonstrated  more  accu-rate  models  in  most  situations  than  Logistic  Regression  and  SVM,  especially  for  classification  with categorical variables.  Further, I do not believe the data will be linearly separable so Logistic Regression is out of the picture, and SVM would require applying kernels to the data to fit non-linearity andis better suited for a higher dimensional space.  Finally,  with a small amount of data,  overfitting isdefinitely a concern, and Random Forest does well to decrease variance with its ensemble technique.

Some  concerns  with  Random  Forest. Although  Random  Forest  is  susceptible  of  bias  towards high cardinality features, I do not believe these are abundant in the data besides ‘hasGender,’ which Ibelieve should have a lot of weight anyway because legendary Pokemon do not usually have a gender.Another concern I had was Random Forest’s inability to deal with imbalanced data due to decisiontrees’ sensitivity to imbalanced data.  However,  the dataset contains 675 non-legendary and 46 leg-endary Pokemon.  A ratio of  15:1 is not severe enough imbalance to take measures against it.  Lastly, strong correlations between variables may cause feature importance to be ”spread” among correlated features, so I believe removing correlated variables is necessary.


DESCRIPTION OF MODEL
NaN Replacement - I replaced NaN values (empty elements in the table) with reasonable alterna-tives for each column:  ‘Type2’ NaNs with ‘Undefined,’ ‘EggGroup2’ NaNs with ‘Undiscovered’ asin ‘EggGroup1,’ ‘PrMale’ NaNs with .5.  PrMale = .5 may seem problematic, but it was my bestoption considering using an invalid number here (outside 0 to 1) would not work for classification, and removing rows with NaN values will only take away hasGender = False columns, which I believe to be crucial for classification. 

Feature Selection - To satiate the aforementioned concerns about multicollinearity,  I first remove correlated  variables  using  the  correlation  matrix,  of  which  I  only  find  that  ‘Generation’  is  highly correlated.  Moreover, I conjecture that many features will not be very useful in determining whethera Pokemon is legendary.  My first thought was dimensionality reduction through Principal ComponentAnalysis (PCA). PCA uses a condensed space (reduces dimensionality to a desired space) and selects the principal components/directions that have maximum variance as features, but I do not believe themost important variables are the ones that have the most variance in them. For example, the ‘Total’ and‘hasGender‘ columns will likely be important (legendary pokemon typically have high Total base statsand don’t have a gender).  ‘Total’ has high variance ranging from 0 to about 700, whereas ‘hasGender’will range from 0 to 1, and thus will not be important in PCA. In general, PCA is not well suited for categorical data and would require heavy rescaling of features.  Further, using Random Forests withoriginal features rather than principal components has the added benefit of allowing me to view howimportant each feature is afterwards.  Thus,  I turned to two Univariate Feature Selection Methods: Chi-Squared  values  for  categorical  data  and  ANOVA  F-values  for  numerical  data  (Although  F-testmakes the assumption that the data are normally distributed, it has been shown to be robust on this assumption).  I picked out ‘Name,’ ‘Type2,’ ‘Color,’ ‘EggGroup2,’ ‘hasMegaEvolution,’ ‘BodyStyle,’and ‘PrMale’ as unnecessary columns.

Random Forest - Initially, I though I would have to perform cross-validation with different important hyperparameters, such as n_estimators, max- depth, and max_features.  Yet, using the defaul tparametes of n_estimators= 100, max_depth=N, and max_features=n_estimators=100 yielded excellent results.


EVALUATION OF MODEL
Since we have a small dataset, cross-validation (CV) seemed best to avoid significant variations in using the conventional single split of the data into training and test sets.  Idecided to use 5-fold and 10-fold CV and recieved the same results for both.  I used the basic metrics of accuracy and standard deviation as a baseline and achieved 0.990 and 0.007 respectively.  Additionally, I calculated the Precision-Recall plot using a Random Forest trained on 80% of the data (I decided against plotting the popular Receiver Operating Characteristic (ROC) because ROC curves tend tobe ”optimistic” for imbalanced datasets). It showcases exceptional results, with an AUC of .966 and F1-Score of .900.  Finally, distinguishing which features are important in legendary classificationis quite interesting.  I use the popular metric of Mean Decrease in Impurity (MDI), which shows the most importance of features by MDI. Here, I cut out less important features, as there were 43 features total with one-hot encoding and the graph would be unreadable otherwise.  It seems that total base stats, gender, and egg group 1 are the most useful features.  These could be used to most efficiently discern if a Pokemon is legendary, especially if data collection is limited. Finally, I’d like to note that I didn’t consider the speed of the algorithm because it was trivial given the small size of the dataset.
